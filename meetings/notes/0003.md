# SIG-Testing meeting notes from 2021-08-27 @ 1830 UTC

Moderator: Kadino
Note Taker: Kadino

## Test Metrics and Reporting

In our last meeting we discussed metrics to capture from the test pipeline, including failure reason. Let's continue that discussion today, but first focus on metrics we want to collect from GitHub Issues.

The motivation for such metrics is to report on the health of the issues database, and help all SIGs drive resolution to their issues. If there is more to say about pipeline metrics topic, let's hold that until after we discuss issue tracking. Let's first discuss metrics such as number of open tickets in a SIG, or the count of new and closed issues within the last week/month. This is an open forum, so feel free to speak up or type ideas in the sig-testing chatroom.

* Thought I remembered a team was working on something about issue label tracking.  Have a scraper that someone had made.  Already a use case and requirements from project management tracking, should be able to use existing requirements.
  * Even if we have something, we can talk about what we want to see here
* Open issues by SIG and by Feature labels, Average Ticket Age
* Does priority exist in GitHub Issues? Tags exist, labels need to be added for prioritization.
* Unlabeled issues will likely need some triage, and a report on unabeled
* We need to make sure metrics are being actioned on - who owns this? SIG-Testing.
  * Should this SIG be driving changes to other SIGs? Other SIGs can passively look at these metrics, but we should notify them when we find a trend.
  * SIG-testing will need to review these metrics? Probably, will discuss with amzn-dk. Periodically a human-driven notification may be important. However we want to provide SIGs the metrics to make their own decisions, and not need to intervene or try to block them.

### Other Metrics

Opening up discussion for all metrics we are interested in, beyond the issues database.

* Is there any kind of code coverage for teams? Currently looking at OpenCPPCoverage which is windows-specific, while SIG teams are not actively asking us for coverage data it seems obvious to add.
  * Often bad ROI to keep adding heavy end-to-end tests, coverage can help focus on unit testing.
  * Code coverage metrics can be hard to compare between larger tests, and harder to action on
  * Code coverage is available locally, but almost nobody seems to collect code coverage information locally because it is extra work.
* Small, efficient tests are important. This points to metrics on test runtime. As well as tests which are marked as non-parallel which must execute in serial.
* How can we scale parallelization through editor tests? 10x optimization on execution time! Planning to standardize this before rolling out to team, want to make sure it is maintainable and as stable as possible. Some tests will need to be non-parallel due to dependencies, and detecting parallel-failures may be tricky.
* What are the next steps for these metrics? Will be work to put together dashboard or notifications. Currently working on getting Amazon-internal metrics and dashboards out to public, may want to adopt Linux Foundation's framework. Meeting with Royal to discuss this next week.

## Wrap up

There is additional work to help the other SIGs plan their tests, need to get SIG-Testing initiatives and issues populated, prioritized.

Currently plan regular SIG-Testing meetings every two weeks at this time, though impromptu meetings are encouraged.

Meeting chatbot cannot record two meetings at once, so no audio recording again this week.
