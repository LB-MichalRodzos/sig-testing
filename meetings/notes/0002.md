# SIG-Testing meeting notes from 2021-08-13 @ 1830 UTC

# Test Metrics and Reporting
This meeting is focused on requirements around what metrics are reported from the Automated Review pipeline. We want to generate requirements so we can plan implementing metrics and reports.

There are a lot of moving parts and blockers, but let's not focus discussion on what is blocked or technical complexity. We can focus on what we want, not when it can be delivered or technical hurdles.

A rough overview of what metrics are already reported:
* Many infrastructural metrics, which fall outside of SIG-Testing: pipeline name, stage name (build, asset process, test, etc.), node name, start time, duration, overall pass/fail, etc.
* Test run pass/fail
* Test run duration, with historic percentile
* Individual test pass/fail
* Individual test history
* Tests per suite, per runner
* Connecting some other metrics now, which are not yet shipped
...Notably the hosting and availability of metrics has not yet been moved into public O3DE from Amazon-internal.

Let's open up to suggestions on what metrics we can record:
* Reporting on pass/fail for each test
* Record suite information
* Break down by functional area (tags)
* Duration of individual tests (per test, not just per module or suite)
* How often, or whether a test is timed-out, and what timed them out (failure reason logging)
* Test duration of living in the Sandbox suite (SLA before being considered abandoned)
* Time that tests stay red after they start failing. This would not be informative for pull requests trying to get through AR. Should be part of branch updates, or of the nightly.
* Benchmark tests record their performance over time.  Will be easier for teams to use this tool when they can easily see trends over time.
* Tagging system for failure reason. Whether the test caught a real issue. Recommend tracking whether the test had a bug in issue tracking database. If we're not storing human-generated test/product failure information, the health dashboard should pull from issues database. If we can pre-determine the failure type (root cause analysis tools) then it can upload a metric, else a human needs to tag issues in the issue tracker.
 * Failure cause is important, in Atom we have added extra lines for Trace::Assert and Trace::Error which show errors that aren't necessarily from the test case. We fail the test case, but this can make a case look incorrectly "flaky" when something else caused it to fail. Have seen environment-specific failures in the EC2 environment, needed this to be able to expose why it was crashing. Was creating perception that is confusing, where it is not related to the test. (tabled a discussion on where failures occur and test complexity)
* There will likely be some type of soak-testing or automated retry feature to search for intermittent failures, will need failure rate reporting specific to intermittent rate.
* Build and test artifacts, need to associate URLs for an engineer to acquire these
* Total number of tests (should already have this)
* Classification of GPU/non-GPU (should already have this)
* Test assets tracking - what can we do here? This could impact the user.
* Asset processing duration (already exists)
* Code coverage, likely to limit to only unit tests (tabled discussion on code coverage hurdles - the metric is still important!)

# Wrap up

We likely needed an hour for this meeting, let's meet again soon on this same subject.








